<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://www.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.example</groupId>
    <artifactId>s3-sink-connector</artifactId>
    <version>1.0-SNAPSHOT</version>

    <repositories>
        <repository>
            <id>confluent</id>
            <url>https://packages.confluent.io/maven/</url>
        </repository>
    </repositories>

    <dependencies>
        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>1.10.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>3.2.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>3.2.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.parquet</groupId>
            <artifactId>parquet-avro</artifactId>
            <version>1.11.1</version>
        </dependency>
        <dependency>
            <groupId>com.amazonaws</groupId>
            <artifactId>aws-java-sdk-s3</artifactId>
            <version>1.12.118</version>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-connect-avro-converter</artifactId>
            <version>5.5.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>connect-api</artifactId>
            <version>2.8.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>2.8.0</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>2.12.4</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.avro</groupId>
                <artifactId>avro-maven-plugin</artifactId>
                <version>1.10.2</version>
                <executions>
                    <execution>
                        <id>avro-schema</id>
                        <goals>
                            <goal>schema</goal>
                        </goals>
                        <phase>generate-sources</phase>
                        <configuration>
                            <imports>
                                <import>C:/Users/ADMIN/Downloads/avro-s3-docker/avro-s3-docker/src/main/resources/LoanRepaymentScheduleCreated.avsc</import>
                            </imports>
                            <enableDecimalLogicalType>true</enableDecimalLogicalType>
                            <sourceDirectory>C:/Users/ADMIN/Downloads/avro-s3-docker/avro-s3-docker/src/main/resources</sourceDirectory>
                            <outputDirectory>C:/Users/ADMIN/Downloads/avro-s3-docker/avro-s3-docker/target/generated-sources</outputDirectory>
                            <fieldVisibility>PRIVATE</fieldVisibility>
                            <stringType>String</stringType>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <createDependencyReducedPom>false</createDependencyReducedPom>
                            <relocations>
                                <relocation>
                                    <pattern>com.fasterxml.jackson</pattern>
                                    <shadedPattern>com.example.shaded.com.fasterxml.jackson</shadedPattern>
                                </relocation>
                            </relocations>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>com.example.S3SinkConnector</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>



package com.example;

import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.example.NonDecomposeFormat; // Ensure the correct import path for NonDecomposeFormat
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.avro.Schema;
import org.apache.kafka.connect.sink.SinkRecord;
import org.apache.kafka.connect.sink.SinkTask;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.ParquetFileWriter;
import org.apache.kafka.connect.header.Header;

import org.apache.kafka.connect.data.Struct;


import java.io.IOException;
import java.nio.file.Files;

import java.text.SimpleDateFormat;
import java.util.*;

public class S3SinkTask extends SinkTask {
    private AmazonS3 s3Client;
    private String bucketName;
    private Map<String, List<SinkRecord>> topicBuffers;
    private Map<String, Long> topicLastFlushTimes;
    private Map<String, String> topicFileKeys;
    private int batchSize;
    private long batchTimeMs;
    private int eventCounter = 0;

    @Override
    public void start(Map<String, String> props) {
        String accessKeyId = props.get(S3SinkConfig.AWS_ACCESS_KEY_ID);
        String secretAccessKey = props.get(S3SinkConfig.AWS_SECRET_ACCESS_KEY);
        bucketName = props.get(S3SinkConfig.S3_BUCKET_NAME);

        BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKeyId, secretAccessKey);
        s3Client = AmazonS3ClientBuilder.standard()
                .withRegion(Regions.fromName(props.get(S3SinkConfig.S3_REGION)))
                .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
                .build();

        topicBuffers = new HashMap<>();
        topicLastFlushTimes = new HashMap<>();
        topicFileKeys = new HashMap<>();

        batchSize = Integer.parseInt(props.get(S3SinkConfig.S3_BATCH_SIZE));
        batchTimeMs = Long.parseLong(props.get(S3SinkConfig.S3_BATCH_TIME_MS));
    }

    @Override
    public void put(Collection<SinkRecord> records) {
        for (SinkRecord record : records) {
            String topic = record.topic();
            topicBuffers.computeIfAbsent(topic, k -> new ArrayList<>()).add(record);

            // Print all headers for inspection
            System.out.println("Headers for record in topic " + topic + ":");
            for (Header header : record.headers()) {
                System.out.println(header.key() + ": " + header.value());
            }

            if (topicBuffers.get(topic).size() >= batchSize || (System.currentTimeMillis() - topicLastFlushTimes.getOrDefault(topic, 0L)) >= batchTimeMs) {
                flushRecords(topic);
                topicFileKeys.put(topic, generateFileKey());
            }
        }
    }

    private void flushRecords(String topic) {
        if (!topicBuffers.get(topic).isEmpty()) {
            try {
                String key = String.format("%s/%s", topic, topicFileKeys.getOrDefault(topic, generateFileKey()));
                java.nio.file.Path tempFile = Files.createTempFile("parquet", ".parquet");

                // Write records to Parquet file
                try (ParquetWriter<NonDecomposeFormat> writer = AvroParquetWriter.<NonDecomposeFormat>builder(new Path(tempFile.toString()))
                        .withSchema(NonDecomposeFormat.getClassSchema())
                        .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)
                        .withCompressionCodec(CompressionCodecName.SNAPPY)
                        .build()) {

                    for (SinkRecord record : topicBuffers.get(topic)) {
                        NonDecomposeFormat avroRecord = createNonDecomposeFormatFromRecord(record);
                        writer.write(avroRecord);
                    }
                }

                // Upload Parquet file to S3
                s3Client.putObject(new PutObjectRequest(bucketName, key, tempFile.toFile()));
                topicBuffers.get(topic).clear();
                topicLastFlushTimes.put(topic, System.currentTimeMillis());
                Files.delete(tempFile);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

//    private NonDecomposeFormat createNonDecomposeFormatFromRecord(SinkRecord record) {
//        NonDecomposeFormat nonDecomposeFormat = new NonDecomposeFormat();
//        nonDecomposeFormat.setEventId(String.valueOf(getHeaderValue(record, "eventId")));
//        nonDecomposeFormat.setPartition(String.valueOf(record.kafkaPartition()));
//        nonDecomposeFormat.setUpdDate(String.valueOf(getHeaderValue(record, "updDate")));
//        nonDecomposeFormat.setPublishTimestampStr(String.valueOf(getHeaderValue(record, "publishTimestampStr")));
//        nonDecomposeFormat.setPayload(record.value().toString());
//        nonDecomposeFormat.setTopic(record.topic());
//        nonDecomposeFormat.setEventName(String.valueOf(getHeaderValue(record, "eventName")));
//        nonDecomposeFormat.setEventVersionNumber(String.valueOf(getHeaderValue(record, "eventVersionNumber")));
//        nonDecomposeFormat.setExecutionTimestampStr(String.valueOf(getHeaderValue(record, "executionTimestampStr")));
//        return nonDecomposeFormat;
//    }
private NonDecomposeFormat createNonDecomposeFormatFromRecord(SinkRecord record) {
    NonDecomposeFormat nonDecomposeFormat = new NonDecomposeFormat();
    nonDecomposeFormat.setEventId(String.valueOf(getHeaderValue(record, "eventId")));
    nonDecomposeFormat.setPartition(String.valueOf(record.kafkaPartition()));
    nonDecomposeFormat.setUpdDate(String.valueOf(getHeaderValue(record, "updDate")));
    nonDecomposeFormat.setPublishTimestampStr(String.valueOf(getHeaderValue(record, "publishTimestampStr")));

    // Convert payload to JSON string
    Struct valueStruct = (Struct) record.value();
    Map<String, Object> payloadMap = new HashMap<>();
    valueStruct.schema().fields().forEach(field -> {
        payloadMap.put(field.name(), valueStruct.get(field));
    });
    ObjectMapper objectMapper = new ObjectMapper();
    String payloadJson;
    try {
        payloadJson = objectMapper.writeValueAsString(payloadMap);
        nonDecomposeFormat.setPayload(payloadJson);
    } catch (JsonProcessingException e) {
        e.printStackTrace();
    }

    nonDecomposeFormat.setTopic(record.topic());
    nonDecomposeFormat.setEventName(String.valueOf(getHeaderValue(record, "eventName")));
    nonDecomposeFormat.setEventVersionNumber(String.valueOf(getHeaderValue(record, "eventVersionNumber")));
    nonDecomposeFormat.setExecutionTimestampStr(String.valueOf(getHeaderValue(record, "executionTimestampStr")));
    return nonDecomposeFormat;
}


    private Object getHeaderValue(SinkRecord record, String headerKey) {
        Header header = record.headers().lastWithName(headerKey);
        return header != null ? header.value() : null;
    }

    private String generateFileKey() {
        String timestamp = new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
        return String.format("event%d-%s.parquet", eventCounter, timestamp);
    }

    @Override
    public void stop() {
        for (String topic : topicBuffers.keySet()) {
            if (!topicBuffers.get(topic).isEmpty()) {
                flushRecords(topic);
            }
        }
    }

    @Override
    public String version() {
        return "1.0";
    }
}
